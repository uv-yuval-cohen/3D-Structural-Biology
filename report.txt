headline:
summary:
intro:

methods:
we go on to talk about methods leading our project.encountaring such problem, we wanted to develop our way into completing our goal. first we started with the data set of 4000 protein, 
a substantaly large amount, especially when a seperation into peptides is needed. After pre processing the data, extracting the peptides in relation to the proteins, and classifing them 
by their true label (nes containment), we moved on to the evaluation. Our strategy was to advance our model with the evaluation of the data acccuracy. processing a substantial amount of the 
proteins, dividing into peptides, embedding them, getting a score and a prediction using a changing threshold and finally getting a classification accuracy. with that the goalmis to progress 
the model, involving a well connected new data of negative examples to train the data better, and then improving our accuracy, everything towards the final goal of predicting the 
20K proteins, whether they contain nes or not. our workflow is based on evaluation and learning into progression, giving us a good balance of testing and correcting.
(chatgpt answer, needs review : Methods
In this project, we aimed to develop a robust and systematic workflow to achieve accurate classification of proteins containing nuclear export signals (NES). Throughout the process, we encountered various challenges due to the scale and complexity of the data, and we continuously refined our approach to address them.
We began by working with a dataset of 4,000 proteins — a substantial volume, particularly considering the need to segment each protein into its corresponding peptides. The first phase involved thorough pre-processing of the dataset: extracting peptides associated with each protein, and accurately labeling these peptides according to their true classification (presence or absence of NES elements).
Following this data preparation, we proceeded to the evaluation stage. Our strategy was to advance the model by iteratively analyzing its performance and predictive accuracy. The processing pipeline involved several key steps:
segmenting proteins into peptides,
generating embeddings for each peptide using the ESM-2 protein language model,
applying a scoring function with a dynamic threshold to produce predictions,
and computing overall classification accuracy at the protein level.
During this phase, we also conducted an additional experiment: we introduced a modification to control the number of peptides required for a protein to be classified as positive (i.e., containing NES). This adjustment was motivated by the observation that some proteins contained a disproportionately large number of peptides, which could bias the classification outcome. By imposing a limit on the minimum number of peptides needed to assign a positive label, we improved classification stability. Although this step was successful, it is considered more of an experimental adjustment rather than a core component of our methods.
A key part of our overall approach was the iterative refinement of the training data. After each evaluation cycle, we enhanced the dataset — in particular, by curating improved sets of negative examples — in order to create a more balanced and representative training set. This process helped the model generalize better and achieve higher accuracy over time.
The final objective of our workflow is to deploy the optimized model on a larger dataset of 20,000 proteins, enabling us to determine, with improved reliability, whether each protein contains NES elements or not. The design of this workflow — centered around evaluation, iterative learning, and correction — provides a well-balanced framework for progressing toward this goal.)


To address the challenge of identifying NES-containing proteins at scale, we developed a structured and iterative workflow grounded in evaluation and continuous improvement.

We began with a dataset of approximately 4,000 human proteins—a substantial volume, particularly given the need to segment each protein into overlapping peptides. In the preprocessing stage, each protein sequence was divided into sliding windows of 22 amino acids, generating a comprehensive set of candidate peptides. These peptides were then labeled according to their known NES status, based on external annotations.

Following this, we proceeded to model evaluation. Our pipeline involved:

Segmenting proteins into peptides,

Generating embeddings for each peptide using a pre-trained protein language model (e.g., ESM),

Applying a scoring function and a tunable threshold to predict NES presence,

Calculating classification performance metrics, including accuracy and AUC.

An important aspect of our approach was the refinement of the dataset between evaluation cycles. By carefully integrating high-quality negative examples, we aimed to improve the model's ability to distinguish NES-containing peptides from similar non-functional sequences. This feedback loop—evaluating, adjusting, and retraining—allowed us to steadily improve prediction quality.

Ultimately, this pipeline is designed to scale up to a larger set of ~20,000 human proteins. Our long-term objective is to accurately classify each protein based on its potential to contain NES elements. The combination of dynamic evaluation, model fine-tuning, and data-driven learning positions our workflow as a robust foundation for large-scale NES prediction.


